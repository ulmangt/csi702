#include <stdlib.h>
#include <stdio.h>
#include <assert.h>

#include "obs_math.h"
#include "filter_constants.h"
#include "filter_cuda_data.h"

#include <thrust/gather.h>
#include <thrust/device_ptr.h>
#include <thrust/reduce.h>
#include <thrust/functional.h>
#include <thrust/transform.h>
#include <thrust/iterator/zip_iterator.h>

#define MAX_RANGE 20000 // meters
#define MAX_VEL 15 // meters per second

#define DEVICE_PI acosf(-1)

// linear congruential random number generator constants
// these values are used in java.util.Random.next()
#define a 0x5DEECE66DL
#define c 0xB
#define mask_bits 48
// generate only 31 bits so that we always generate positive integers
#define result_bits 31

// bit mask the least significant m bits
const long mask = ( 1L << mask_bits ) - 1;

const long LCG_RAND_MAX = ( 1L << result_bits ) - 1;

// implementation based on examples from:
// http://en.wikipedia.org/wiki/Linear_congruential_generator
// java.util.Random.next()
__device__ int device_lcg_rand( int x )
{
  // bitwise and with mask is equivalent to mod 2^mask_bits
  long xl = (x * a + c) & mask;
  // we have generated mask_bits but only need result_bits
  // use the highest order bits because they have longer periods
  return (int) ( xl >> ( mask_bits - result_bits ) );
}

// Return a random float value evenly distributed between 0 and max
// Because cuda threads will keep track of their own seeds, this function
// and the others like it are not random. They simply take an already
// generated random value (seed) and transform it in some way.
// Seed values should be generated by repeated calls to device_lcg_rand().
__device__ float device_frand0( int seed, float max )
{
  return ( float ) seed / ( float ) LCG_RAND_MAX * max ;
}

// return a random float value evenly distributed between min and max
__device__ float device_frand( int seed, float min, float max )
{
  float diff = max - min;
  return device_frand0( seed, diff ) + min;
}

// return an exponentially distributed random float
__device__ float device_erand( int seed, float inv_lambda )
{
  return -log( device_frand0( seed, 1.0 ) ) * inv_lambda;
}


void checkCUDAError(const char *msg)
{
    cudaError_t err = cudaGetLastError();
    if( cudaSuccess != err) 
    {
        fprintf(stderr, "Cuda error: %s: %s.\n", msg, cudaGetErrorString( err) );
        exit(-1);
    }
}




// CUDA kernel function : time update a particle
__global__ void time_update_kernel( struct particles device_particles, float time_sec, float mean_maneuver )
{
  // get the current particle index and retrieve the particle
  int index = blockIdx.x * blockDim.x + threadIdx.x;

  // store the particle's random seed
  float seed = device_particles.seed[index];

  // use the random seed to make a random draw from an exponential distribution with mean time_sec
  // if the draw is larger than mean_maneuver, the particle maneuvers
  if ( device_erand( seed, time_sec ) > mean_maneuver )
  {
    seed = device_lcg_rand( seed );
    device_particles.x_vel[index] = device_particles.x_vel[index] + device_frand( seed, -MAX_VEL_PERTURB, MAX_VEL_PERTURB );
    seed = device_lcg_rand( seed );
    device_particles.y_vel[index] = device_particles.y_vel[index] + device_frand( seed, -MAX_VEL_PERTURB, MAX_VEL_PERTURB );
  }

  // update the random seed and store it back in the particle storage
  device_particles.seed[index] = device_lcg_rand( seed );

  // update the particle's position and velocity
  device_particles.x_pos[index] = device_particles.x_pos[index] + device_particles.x_vel[index] * time_sec;
  device_particles.y_pos[index] = device_particles.y_pos[index] + device_particles.y_vel[index] * time_sec;
}

// updates the position of all particles based on their current velocity
extern "C" void time_update( struct particles device_particles, int num, float time_sec, float mean_maneuver )
{
  int numBlocks = num / THREADS_PER_BLOCK;

  // launch kernel
  dim3 dimGrid(numBlocks);
  dim3 dimBlock(THREADS_PER_BLOCK);
  time_update_kernel<<< dimGrid, dimBlock >>>( device_particles, time_sec, mean_maneuver );

  // block until the device has completed kernel execution
  cudaThreadSynchronize();

  // check if the init_particle_val kernel generated errors
  checkCUDAError("time_update");
}





// CUDA kernel function : initialize a particle
__global__ void init_particles_kernel( struct particles device_particles )
{
  int index = blockIdx.x * blockDim.x + threadIdx.x;

  int seed = device_particles.seed[index];

  device_particles.x_pos[index]  = device_frand( seed, -MAX_RANGE, MAX_RANGE );
  seed = device_lcg_rand( seed );
  device_particles.y_pos[index]  = device_frand( seed, -MAX_RANGE, MAX_RANGE );
  seed = device_lcg_rand( seed );
  device_particles.x_vel[index]  = device_frand( seed, -MAX_VEL, MAX_VEL );
  seed = device_lcg_rand( seed );
  device_particles.y_vel[index]  = device_frand( seed, -MAX_VEL, MAX_VEL );
  seed = device_lcg_rand( seed );
  device_particles.weight[index] = 2.0;

  device_particles.seed[index] = seed;
}

// initialize particles, use random seeds to set random positions and velocities
// also set weights of all particles to 1.0
extern "C" void init_particles( struct particles device_particles, int num )
{
  int numBlocks = num / THREADS_PER_BLOCK;

  // launch kernel
  dim3 dimGrid(numBlocks);
  dim3 dimBlock(THREADS_PER_BLOCK);
  init_particles_kernel<<< dimGrid, dimBlock >>>( device_particles );

  // block until the device has completed kernel execution
  cudaThreadSynchronize();

  // check if the init_particle_val kernel generated errors
  checkCUDAError("init_particles");
}



// modified from reduction code example in CUDA sdk
// num must be a power of 2 for this routine to function
__global__ void sum_weight_array_kernel( float *d_weight_in, float *d_weight_out, int num )
{
  // allocate a shared memory array the size of the current block
  __shared__ float s_shared[THREADS_PER_BLOCK];

  int tid = threadIdx.x; // thread id within block

  // similar to the standard global array index calculation
  // except that blockDimx.x sized spaces are left between
  // the indexes assigned to the threads in each block
  int i = blockIdx.x * ( blockDim.x * 2 ) + threadIdx.x;

  // copy weights from global memory into shared memory for
  // num / 2 threads (only half the threads are from blocks
  // whose i indexes corrispond to actual indexes)
  s_shared[ tid ] = i < num ? d_weight_in[ i ] : 0 ;

  // perform the first stage of the reduction reading from global memory
  // threads in block n add the weight of their corrisponding particle
  // in block n + 1, half the threads sit idle
  if ( i + blockDim.x < num )
  {
    s_shared[ tid ] += d_weight_in[ i + blockDim.x ];
  }

  // wait for s_shared to be fully populated
  __syncthreads();

  // offset tracks the distance between the two entries from
  // s_shared that are added together in the current iteration
  // also, each iteration, offset threads are used
  unsigned int offset = blockDim.x / 2;
  while ( offset > 32 )
  {
    if ( tid < offset )
    {
      s_shared[ tid ] += s_shared[ tid + offset ];
    }

    offset = offset >> 1; // divide offset by 2

    __syncthreads();
  }

  // after offset is 32, all our threads are working within a single warp
  // this means all operations they perform are SIMD (simultanious) and
  // require no __syncthreads()
  // thus, each line corrisponds to an unrolled iteration of the above loop 
  if ( tid < 32 )
  {
    s_shared[ tid ] += s_shared[ tid + 32 ];
    s_shared[ tid ] += s_shared[ tid + 16 ];
    s_shared[ tid ] += s_shared[ tid + 8 ];
    s_shared[ tid ] += s_shared[ tid + 4 ];
    s_shared[ tid ] += s_shared[ tid + 2 ];
    s_shared[ tid ] += s_shared[ tid + 1 ];
  }

  // s_shared[0] now contains the final weight for this block
  // write that result to the global weights array position
  // corrisponding to this block's id
  if ( tid == 0 )
  {
    d_weight_out[ blockIdx.x ] = s_shared[ 0 ];
  }
}

// final reduction routine optimized to work with a single block
__global__ void sum_weight_final_kernel( float *d_weight_in, float *d_weight_out, int num )
{
  // allocate a shared memory array the size of the current block
  __shared__ float s_shared[THREADS_PER_BLOCK];

  int tid = threadIdx.x; // thread id within block

  s_shared[ tid ] = d_weight_in[ tid ] ;

  // offset tracks the distance between the two entries from
  // s_shared that are added together in the current iteration
  // also, each iteration, offset threads are used
  unsigned int offset = blockDim.x / 2;
  while ( offset > 0 )
  {
    if ( tid < offset )
    {
      s_shared[ tid ] += s_shared[ tid + offset ];
    }

    offset = offset >> 1; // divide offset by 2

    __syncthreads();
  }

  // s_shatred[0] now contains the final weight for this block
  // write that result to the global weights array position 0
  if ( tid == 0 )
  {
    d_weight_out[ 0 ] = s_shared[ 0 ];
  }
}

extern "C" float sum_weight( float *d_weights, float *d_temp_weight_in, float *d_temp_weight_out, int num )
{
  int numBlocks = num / THREADS_PER_BLOCK;
  int shared_mem_size = sizeof( float ) * THREADS_PER_BLOCK;

  // copy the particles into a temporary array
  cudaMemcpy( d_temp_weight_in, d_weights, num * sizeof(float), cudaMemcpyDeviceToDevice );

  // each iteration d_temp_weight_out is populated with numBlocks weights
  // for the next iteration, numBlocks is used as numThreads until
  // numBlocks is less than 512, in which case the next iteration
  // can finish the reduction using a single block
  do
  {
    //printf("running kernel blocks: %d num: %d smsize: %d \n", numBlocks, num, shared_mem_size );

    dim3 dimGrid(numBlocks);
    dim3 dimBlock(THREADS_PER_BLOCK);
    sum_weight_array_kernel<<< dimGrid, dimBlock, shared_mem_size >>>( d_temp_weight_in, d_temp_weight_out, num );

    // block until the device has completed kernel execution
    cudaThreadSynchronize();

    // check if the init_particle_val kernel generated errors
    checkCUDAError("sum_weight");

    // for the next iteration, use the out array as the new in array
    float *temp = d_temp_weight_out;
    d_temp_weight_out = d_temp_weight_in;
    d_temp_weight_in = temp;

    // update array size and block count
    num = numBlocks;
    numBlocks = num / THREADS_PER_BLOCK;

    //printf("finished kernel blocks: %d num: %d smsize: %d \n", numBlocks, num, shared_mem_size );
  }
  while( num > THREADS_PER_BLOCK );

  //printf("running final kernel block size: %d \n", numBlocks );

  dim3 dimGridFinal(1);
  dim3 dimBlockFinal(num);
  sum_weight_final_kernel<<< dimGridFinal, dimBlockFinal, shared_mem_size >>>( d_temp_weight_in, d_temp_weight_out, shared_mem_size );

  // block until the device has completed kernel execution
  cudaThreadSynchronize();

  // check if the init_particle_val kernel generated errors
  checkCUDAError("sum_weight");

  float final_sum = 0;

  // each block has reported its sum, now copy those block sums
  // back to the host and sum the blocks weight sums in serial
  // remembering that only half the entries contain partial sums
  cudaMemcpy( &final_sum, d_temp_weight_out, sizeof(float), cudaMemcpyDeviceToHost );

  return final_sum;
}

extern "C" float sum_weight_thrust( float *d_weights, int num )
{
  thrust::device_ptr<float> device_data( d_weights );
  float sum = thrust::reduce(device_data, device_data + num, 0.0f, thrust::plus<float>());
  return sum;
}



// device function to calculate the distance between two particles
__device__ float range( float to_x_pos, float to_y_pos, float from_x_pos, float from_y_pos )
{
  float x_diff = from_x_pos - to_x_pos;
  float y_diff = from_y_pos - to_y_pos;

  return sqrt( x_diff * x_diff + y_diff * y_diff );
}

// device function to calculate the azimuth between two particles
__device__ float azimuth( float to_x_pos, float to_y_pos, float from_x_pos, float from_y_pos )
{
  float x_diff = from_x_pos - to_x_pos;
  float y_diff = from_y_pos - to_y_pos;

  if ( x_diff == 0 && y_diff > 0 ) return DEVICE_PI / 2.0;
  if ( x_diff == 0 && y_diff < 0 ) return -DEVICE_PI / 2.0;

  return atan2( y_diff, x_diff );
}

// calculates the probability density function for the gaussian
// distribution with given mean and sigma
__device__ float gvalue( float value, float mean, float sigma )
{
  float z = ( value - mean ) / sigma ;
  return exp( -0.5 * z * z ) / ( sqrt( 2.0 * DEVICE_PI ) * sigma );
}

// CUDA kernel function : apply an azimuth observation (adjust particle weight)
__global__ void apply_azimuth_observation_kernel( struct particles device_particles,
                                                  struct observation obs )
{
  int index = blockIdx.x * blockDim.x + threadIdx.x;

  float particle_azimuth = azimuth( obs.x_pos , obs.y_pos , device_particles.x_pos[index] , device_particles.y_pos[index] );
  float likelihood = gvalue( particle_azimuth - obs.value , 0.0 , obs.error );

  device_particles.weight[index] = device_particles.weight[index] * likelihood;
}

// CUDA kernel function : apply a range observation (adjust particle weight)
__global__ void apply_range_observation_kernel( struct particles device_particles,
                                                struct observation obs )
{
  int index = blockIdx.x * blockDim.x + threadIdx.x;

  float particle_range = range( obs.x_pos , obs.y_pos , device_particles.x_pos[index] , device_particles.y_pos[index] );
  float likelihood = gvalue( particle_range - obs.value , 0.0 , obs.error );

  device_particles.weight[index] = device_particles.weight[index] * likelihood;
}

// apply observation obs to the particle list, adjusting particle weights
extern "C" void information_update( struct observation obs, struct particles device_particles, int num )
{
  int numBlocks = num / THREADS_PER_BLOCK;

  // launch kernel
  dim3 dimGrid(numBlocks);
  dim3 dimBlock(THREADS_PER_BLOCK);

  switch( obs.type )
  {
    case AZIMUTH:
      apply_azimuth_observation_kernel<<< dimGrid, dimBlock >>>( device_particles, obs );
      break;
    case RANGE:
      apply_range_observation_kernel<<< dimGrid, dimBlock >>>( device_particles, obs );
      break;
  }

  // block until the device has completed kernel execution
  cudaThreadSynchronize();

  // check if the init_particle_val kernel generated errors
  checkCUDAError("information_update");
}




// CUDA kernel function : multiply an array by a constant value
__global__ void multiply_kernel( float *array, float factor )
{
  int index = blockIdx.x * blockDim.x + threadIdx.x;
  array[index] = array[index] * factor;
}

extern "C" void multiply( float *array, float factor, int num )
{
  int numBlocks = num / THREADS_PER_BLOCK;

  dim3 dimGrid(numBlocks);
  dim3 dimBlock(THREADS_PER_BLOCK);
  multiply_kernel<<< dimGrid, dimBlock >>>( array, factor );

  // block until the device has completed kernel execution
  cudaThreadSynchronize();

  // check if the kernel generated errors
  checkCUDAError("multiply");
}




__global__ void init_array_kernel( float *array, float value )
{
  int index = blockIdx.x * blockDim.x + threadIdx.x;
  array[index] = value;
}

extern "C" void init_array( float *array, float value, int num )
{
  int numBlocks = num / THREADS_PER_BLOCK;

  dim3 dimGrid(numBlocks);
  dim3 dimBlock(THREADS_PER_BLOCK);
  init_array_kernel<<< dimGrid, dimBlock >>>( array, value );

  // block until the device has completed kernel execution
  cudaThreadSynchronize();

  // check if the kernel generated errors
  checkCUDAError("init_array");
}




__global__ void floor_array_kernel( float *array )
{
  int index = blockIdx.x * blockDim.x + threadIdx.x;
  array[index] = floor(array[index]);
}

extern "C" void floor_array( float *array, int num )
{
  int numBlocks = num / THREADS_PER_BLOCK;

  dim3 dimGrid(numBlocks);
  dim3 dimBlock(THREADS_PER_BLOCK);
  floor_array_kernel<<< dimGrid, dimBlock >>>( array );

  // block until the device has completed kernel execution
  cudaThreadSynchronize();

  // check if the kernel generated errors
  checkCUDAError("floor_array");
}


__global__ void perturb_particles_v3_kernel( float *seeds, struct particles device_array_swap )
{
  int index = blockIdx.x * blockDim.x + threadIdx.x;

  int seed = seeds[index];
  device_array_swap.x_pos[index] = device_array_swap.x_pos[index] + device_frand( seed, -MAX_POS_PERTURB, MAX_POS_PERTURB );
  seed = device_lcg_rand( seed );
  device_array_swap.y_pos[index] = device_array_swap.y_pos[index] + device_frand( seed, -MAX_POS_PERTURB, MAX_POS_PERTURB );
  seed = device_lcg_rand( seed );
  device_array_swap.x_vel[index] = device_array_swap.x_vel[index] + device_frand( seed, -MAX_VEL_PERTURB, MAX_VEL_PERTURB );
  seed = device_lcg_rand( seed );
  device_array_swap.y_vel[index] = device_array_swap.y_vel[index] + device_frand( seed, -MAX_VEL_PERTURB, MAX_VEL_PERTURB );
  seed = device_lcg_rand( seed );
  device_array_swap.seed[index] = seed;
}

extern "C" void perturb_particles_v3( float *seeds, struct particles device_array_swap, int num )
{
  int numBlocks = num / THREADS_PER_BLOCK;

  dim3 dimGrid(numBlocks);
  dim3 dimBlock(THREADS_PER_BLOCK);
  perturb_particles_v3_kernel<<< dimGrid, dimBlock >>>( seeds, device_array_swap );

  // block until the device has completed kernel execution
  cudaThreadSynchronize();

  // check if the kernel generated errors
  checkCUDAError("perturb_particles_v3");
}


__global__ void cap_array_v3_kernel( float *device_array, float value )
{
  int index = blockIdx.x * blockDim.x + threadIdx.x;

  if ( device_array[index] >= value )
  {
    device_array[index] = 0;
  }
  
}

extern "C" void cap_array_v3( float *device_array, float value, int num )
{
  int numBlocks = num / THREADS_PER_BLOCK;

  dim3 dimGrid(numBlocks);
  dim3 dimBlock(THREADS_PER_BLOCK);
  cap_array_v3_kernel<<< dimGrid, dimBlock >>>( device_array, value );

  // block until the device has completed kernel execution
  cudaThreadSynchronize();

  // check if the kernel generated errors
  checkCUDAError("cap_array_v");
}



// device function to copy and perturb a single particle
__device__ void copy_particle( int from_index, int to_index,
                               struct particles device_array,
                               struct particles device_array_swap )
{
  // copy_particle is called by the thread corrisponding to from_index
  // for each to_index in serial, thus we get the seed from from_index
  // and store it back so the next iteration can use the new seed
  // (this is quite inefficient)
  int seed = device_array.seed[from_index];
  device_array_swap.x_pos[to_index] = device_array.x_pos[from_index] + device_frand( seed, -MAX_POS_PERTURB, MAX_POS_PERTURB );
  seed = device_lcg_rand( seed );
  device_array_swap.y_pos[to_index] = device_array.y_pos[from_index] + device_frand( seed, -MAX_POS_PERTURB, MAX_POS_PERTURB );
  seed = device_lcg_rand( seed );
  device_array_swap.x_vel[to_index] = device_array.x_vel[from_index] + device_frand( seed, -MAX_VEL_PERTURB, MAX_VEL_PERTURB );
  seed = device_lcg_rand( seed );
  device_array_swap.y_vel[to_index] = device_array.y_vel[from_index] + device_frand( seed, -MAX_VEL_PERTURB, MAX_VEL_PERTURB );
  seed = device_lcg_rand( seed );
  device_array.seed[from_index] = seed;
  device_array_swap.seed[to_index] = seed;
}

// device function to copy and perturb a single particle
__device__ void copy_particle_v2( int from_index, int to_index,
                               struct particles device_array,
                               struct particles device_array_swap )
{
  // copy_particle_v2 is called by the thread corrisponding to to_index
  // thus we get the seed to use from that index
  int seed = device_array.seed[to_index];
  device_array_swap.x_pos[to_index] = device_array.x_pos[from_index] + device_frand( seed, -MAX_POS_PERTURB, MAX_POS_PERTURB );
  seed = device_lcg_rand( seed );
  device_array_swap.y_pos[to_index] = device_array.y_pos[from_index] + device_frand( seed, -MAX_POS_PERTURB, MAX_POS_PERTURB );
  seed = device_lcg_rand( seed );
  device_array_swap.x_vel[to_index] = device_array.x_vel[from_index] + device_frand( seed, -MAX_VEL_PERTURB, MAX_VEL_PERTURB );
  seed = device_lcg_rand( seed );
  device_array_swap.y_vel[to_index] = device_array.y_vel[from_index] + device_frand( seed, -MAX_VEL_PERTURB, MAX_VEL_PERTURB );
  seed = device_lcg_rand( seed );
  device_array_swap.seed[to_index] = seed;
}

// prior to calling this kernel, the particle weights have been overwritten with:
//
// weight[i] = weight[i] * ( length( weight[] / sum( weight[] ) )
//
// this value (ignorring the decimal component for the moment) tells us aproximately
// how many particles this particle should be resampled into. The trick is to also
// determine where the resampled particles should go, so the resampling can proceed
// in paralle. This is accomplished by further modifying the weight:
//
// weight[i] = floor( cumulative_sum( weight[i] ) )
//
// Now, weight[i-1] gives the index into the global memory particle array that copies
// of this particle should be placed in and weight[i] - weight[i-1] gives the number
// of particle this particle should be resampled into.
__global__ void copy_particles_kernel( struct particles device_array,
                                       struct particles device_array_swap )
{
  int index = blockIdx.x * blockDim.x + threadIdx.x;

  // this is coalesced memory access
  int resample_start_index = index == 0 ? 0 : (int) device_array.weight[index-1];
  int resample_num_copies = device_array.weight[index] - resample_start_index;

  int i;
  for ( i = 0 ; i < resample_num_copies ; i++ )
  {
    copy_particle( index, resample_start_index + i, device_array, device_array_swap );
  }  
}

extern "C" void copy_particles( struct particles device_array,
                                struct particles device_array_swap,
                                int num )
{
  int numBlocks = num / THREADS_PER_BLOCK;

  dim3 dimGrid(numBlocks);
  dim3 dimBlock(THREADS_PER_BLOCK);
  copy_particles_kernel<<< dimGrid, dimBlock >>>( device_array, device_array_swap );

  // block until the device has completed kernel execution
  cudaThreadSynchronize();

  // check if the kernel generated errors
  checkCUDAError("copy_particles");
}


__global__ void copy_indexes_kernel( struct particles device_array,
                                     struct particles device_array_swap )
{
  int index = blockIdx.x * blockDim.x + threadIdx.x;

  // this is coalesced memory access
  int resample_start_index = index == 0 ? 0 : (int) device_array.weight[index-1];
  int resample_num_copies = device_array.weight[index] - resample_start_index;

  int i;
  for ( i = 0 ; i < resample_num_copies ; i++ )
  {
    device_array_swap.weight[ resample_start_index + i ] = index;
  }  
}

extern "C" void copy_indexes( struct particles device_array,
                              struct particles device_array_swap,
                              int num )
{
  int numBlocks = num / THREADS_PER_BLOCK;

  dim3 dimGrid(numBlocks);
  dim3 dimBlock(THREADS_PER_BLOCK);
  copy_indexes_kernel<<< dimGrid, dimBlock >>>( device_array, device_array_swap );

  // block until the device has completed kernel execution
  cudaThreadSynchronize();

  // check if the kernel generated errors
  checkCUDAError("copy_indexes");
}

__global__ void copy_particles_v2_kernel( struct particles device_array,
                                          struct particles device_array_swap,
                                          int num )
{
  int index = blockIdx.x * blockDim.x + threadIdx.x;

  int copy_from_index = (int) device_array_swap.weight[index];

  if ( copy_from_index < num )
    copy_particle_v2( copy_from_index, index, device_array, device_array_swap );
}

extern "C" void copy_particles_v2(  struct particles device_array,
                                    struct particles device_array_swap,
                                    int num )
{
  int numBlocks = num / THREADS_PER_BLOCK;

  dim3 dimGrid(numBlocks);
  dim3 dimBlock(THREADS_PER_BLOCK);
  copy_particles_v2_kernel<<< dimGrid, dimBlock >>>( device_array, device_array_swap, num );

  // block until the device has completed kernel execution
  cudaThreadSynchronize();

  // check if the kernel generated errors
  checkCUDAError("copy_particles_v2");
}

__global__ void copy_float_to_int_kernel( float *from, int *to )
{
  int index = blockIdx.x * blockDim.x + threadIdx.x;

  to[index] = (int) from[index];
}

extern "C" void copy_float_to_int(  float *from, int *to, int num )
{
  int numBlocks = num / THREADS_PER_BLOCK;

  dim3 dimGrid(numBlocks);
  dim3 dimBlock(THREADS_PER_BLOCK);
  copy_float_to_int_kernel<<< dimGrid, dimBlock >>>( from, to );

  // block until the device has completed kernel execution
  cudaThreadSynchronize();

  // check if the kernel generated errors
  checkCUDAError("copy_float_to_int");
}


__global__ void calc_effective_particle_count_helper_kernel( float *d_weights, float *d_weights_swap, float weight_sum )
{
  int index = blockIdx.x * blockDim.x + threadIdx.x;

  float weight = d_weights[index] / weight_sum ;

  d_weights_swap[index] = weight * weight;
}

extern "C" void calc_effective_particle_count_helper( float *d_weights, float *d_weights_swap, float weight_sum, int num )
{
  int numBlocks = num / THREADS_PER_BLOCK;

  dim3 dimGrid(numBlocks);
  dim3 dimBlock(THREADS_PER_BLOCK);
  calc_effective_particle_count_helper_kernel<<< dimGrid, dimBlock >>>( d_weights, d_weights_swap, weight_sum );

  // block until the device has completed kernel execution
  cudaThreadSynchronize();

  // check if the kernel generated errors
  checkCUDAError("calc_effective_particle_count");
}


extern "C" float calc_effective_particle_count_thrust( float *d_weights, float *d_weights_swap, float weight_sum, int num )
{
  thrust::device_ptr<float> device_data( d_weights );
  float sum = thrust::reduce(device_data, device_data + num, 0.0f, thrust::plus<float>());
}

// provides an estimate of the total number of particles
// which are actually contributing information to the distribution
// see: http://en.wikipedia.org/wiki/Particle_filter#Sampling_Importance_Resampling_.28SIR.29
extern "C" float calc_effective_particle_count( struct particles device_array,
                                                struct particles device_array_swap,
                                                int num )
{
  float weight_sum = sum_weight_thrust( device_array.weight, num );
  float effective_count = calc_effective_particle_count_thrust( device_array.weight, device_array_swap.weight, weight_sum, num );
}

extern "C" void resample( struct particles device_array,
                          struct particles device_array_swap,
                          int num )
{
  // renormalize weights then multiply by NUM_PARTICLES
  // weight now contains aproximately the number of particles each particle should be resampled into
  // to values near 0 indicating that particle should be removed
  float weight_sum = sum_weight_thrust( device_array.weight, num );
  multiply( device_array.weight, (float) num / weight_sum, num );

  // wrap arrays in thrust data structures
  thrust::device_ptr<float> device_weights( device_array.weight );

  // repace weights with cumulative sum of weights
  thrust::inclusive_scan(device_weights, device_weights + num, device_weights);

  // convert cumulative weights to array indexes
  floor_array( device_array.weight, num );

  // make copies of particles
  copy_particles( device_array, device_array_swap, num );

  // reset all particle weights to 1.0
  init_array( device_array_swap.weight, 1.0f, num );
}

extern "C" void resample_v2( struct particles device_array,
                             struct particles device_array_swap,
                             int num )
{
  // renormalize weights then multiply by NUM_PARTICLES
  // weight now contains aproximately the number of particles each particle should be resampled into
  // to values near 0 indicating that particle should be removed
  float weight_sum = sum_weight_thrust( device_array.weight, num );
  multiply( device_array.weight, (float) num / weight_sum, num );

  // wrap arrays in thrust data structures
  thrust::device_ptr<float> device_weights( device_array.weight );

  // repace weights with cumulative sum of weights
  thrust::inclusive_scan(device_weights, device_weights + num, device_weights);

  // convert cumulative weights to array indexes
  floor_array( device_array.weight, num );

  // copy the index of the particle which will overwrite it into each particle's weight
  copy_indexes( device_array, device_array_swap, num );

  // overwrite/copy particles based on the weights set above
  copy_particles_v2( device_array, device_array_swap, num);

  // reset all particle weights to 1.0
  init_array( device_array_swap.weight, 1.0f, num );
}

extern "C" void resample_v3( struct particles device_array,
                             struct particles device_array_swap,
                             int num )
{
  // renormalize weights then multiply by NUM_PARTICLES
  // weight now contains aproximately the number of particles each particle should be resampled into
  // to values near 0 indicating that particle should be removed
  float weight_sum = sum_weight_thrust( device_array.weight, num );
  multiply( device_array.weight, (float) num / weight_sum, num );

  // wrap arrays in thrust data structures
  thrust::device_ptr<float> device_weights( device_array.weight );

  // repace weights with cumulative sum of weights
  thrust::inclusive_scan(device_weights, device_weights + num, device_weights);

  // convert cumulative weights to array indexes
  floor_array( device_array.weight, num );

  // copy the index of the particle which will overwrite it into each particle's weight
  copy_indexes( device_array, device_array_swap, num );

  // wrap arrays in thrust data structures
  thrust::device_ptr<float> device_weight_swap( device_array_swap.weight );
  thrust::device_ptr<float> device_x_pos( device_array.x_pos );
  thrust::device_ptr<float> device_x_pos_swap( device_array_swap.x_pos );
  thrust::device_ptr<float> device_y_pos( device_array.y_pos );
  thrust::device_ptr<float> device_y_pos_swap( device_array_swap.y_pos );
  thrust::device_ptr<float> device_x_vel( device_array.x_vel );
  thrust::device_ptr<float> device_x_vel_swap( device_array_swap.x_vel );
  thrust::device_ptr<float> device_y_vel( device_array.y_vel );
  thrust::device_ptr<float> device_y_vel_swap( device_array_swap.y_vel );

  // use a thrust 'gather' to copy data from indexes calculated by copy_indexes_kernel
  // see: http://thrust.googlecode.com/svn/tags/1.1.0/doc/html/group__gathering.html
  // for an expanation of the thrust::deprecated::gather namespace, see:
  // http://groups.google.com/group/thrust-users/browse_thread/thread/f5f0583cb97b51fd/38b9550437e0989c?lnk=gst&q=gather#38b9550437e0989c
  thrust::next::gather(device_weight_swap, device_weight_swap+num,
                       thrust::make_zip_iterator(make_tuple(device_x_pos, device_y_pos, device_x_vel, device_y_vel)),
                       thrust::make_zip_iterator(make_tuple(device_x_pos_swap, device_y_pos_swap, device_x_vel_swap, device_y_vel_swap)) );

  // thrust::gather only copies particles, perturbing must be performed in a separate step
  perturb_particles_v3( device_array.seed, device_array_swap, num );

  // reset all particle weights to 1.0
  init_array( device_array_swap.weight, 1.0f, num );
}


// copy particles from host (cpu) to device (video card)
extern "C" void copy_array_host_to_device( float *host, float *device, int num )
{
  int size = sizeof( float ) * num;

  cudaMemcpy( device, host, size, cudaMemcpyHostToDevice );
}

// copy particles from device (video card) to host (cpu)
extern "C" void copy_array_device_to_host( float *host, float *device, int num )
{
  int size = sizeof( float ) * num;

  cudaMemcpy( host, device, size, cudaMemcpyDeviceToHost );
}

// allocate memory for num particles on host
extern "C" float* h_init_array_mem( int num )
{
  int size = sizeof( float ) * num ;
  return ( float * ) malloc( size );
}

// allocate memory for num particles on device
extern "C" void d_init_array_mem( float **array, int num )
{
  int size = sizeof( float ) * num ;

  cudaMalloc( (void **) array, size );
}

// allocate memory for num particles on host
extern "C" struct particles* h_init_particles_mem( )
{
  int size = sizeof( struct particles ) ;
  return ( struct particles * ) malloc( size );
}

// allocate memory for num particles on device
extern "C" void d_init_particles_mem( struct particles **device_particles )
{
  int size = sizeof( struct particles ) ;

  cudaMalloc( (void **) device_particles, size );
}

// free particle memory on host
extern "C" void h_free_mem( void *list )
{
  free( list );
}

// free particle memory on device
extern "C" void d_free_mem( void *list )
{
  cudaFree( list );
}






