\documentclass{article}

\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{graphicx}

\begin{document}

\title{Bayesian Particle Filter Tracking with CUDA}
\author{Geoffrey Ulman\\
        CSI702}
\date{April 2010}
\maketitle

\tableofcontents

\section{Background}\label{Background}
A simple geographic tracking problem traditionally consists of estimating the state of a target using errored observations of some function of the target state. The problem explored here assumes a four-dimensional state space with two cartesian position dimensions and two veocity dimensions. To simplify the problem, the possibility of false alarms (observations which do not corrispond to an actual target) is ignorred. Further, all observations are assumed to corrispond to a single target of interest (associating observations with the correct target track is an additional complicating concern in multi-target tracking problems).

\subsection{Prior Distribution}
Employing Bayesian tracking to estimate the true state \(x \in S\) of a target in state space \(S\) requires a prior distribution \(p(x)\). This probability density function describes the probability that the target's true state is \(x\) prior to receving any observations on the target. This prior distribution is generally based on engineering knowledge of the targets and of the sensors used to generate observations. The simple priors used in this tracking problem assume the sensor has a maximum detection range and thus that targets will not be detected until they are inside that range. The velocity dimension of the target state is also restricted by the known maximum speed of the target being tracked. Together, these form a simple uniform bounded prior distribution on the two position and two velocity state space dimensions. Figure \ref{prior} displays the x and y position components of a random sample of particles drawn from the prior described above with a 20000 meter maximum detection range. Because all particles are initially weighted equally, any particle has an equal likelihood of being the true state of the target.

\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{data/particles_prior.png}
\caption{Prior Particle Position Distribution}
\label{prior}
\end{figure}

\subsection{Likelihood Functions}
A likelihood function \(L\) for random variable \(X\) which takes on value in a state space \(S\) and observation \(Y=y\) on some function of the state space is defined in Equation \ref{lfunc1}. The function \(P(\cdot|x)\) is a probability density function describing the probability of various observations \(y\) given a known target state \(x\).

However, in Bayesian tracking, the function \(P(y|\cdot)\) is far more interesting. This is because once an observation \(y\) is received it is fixed and we wish to determine what that observation tells us about the true target state \(x\). The name likelihood function arises from the fact that if \(L(y|x_{1})>L(y|x_{2})\) then the observation \(y\) is more liekly to have come from a target with state \(x_{1}\) than a target with state \(x_{2}\). Is should be noted that unlike \(P(\cdot|x)\), the likelihood function usually not a proability density function\cite{bmtt}.

\begin{equation}\label{lfunc1}
L(y|x) = P( Y=y | X=x ) \verb! for ! x \in S
\end{equation}

The likelihood function, in conjunction with Bayes' rule, tells us how to modify our prior distribution \(p(x)\) (discussed in Section \ref{Background}) to incorporate the information from an observation \(y\). Applying Bayes' rule we arrive at Equation \ref{lfunc2}. The probability density function \(P(\cdot|y)\) is known as the posterior distribution and represents the prior adjusted to reflect the addition of the new information contained in the observation \(y\). This posterior distribution can have further likelihood functions applied to it using Function \ref{lfunc2} to incorporate additional information. This process is knwon as Bayesian tracking.

\begin{equation}\label{lfunc2}
P(x|y) = \frac{L(y|x)p(x)}{\int \! L(y|x)p(x) \, dx}
\end{equation}

Figure \ref{posterior1} shows the posterior distribution after the likelihood for a single azimuth observation is applied to the prior distribution from Figure \ref{prior}. An azimuth observation is a single angle value indicating that the target is somewhere along the ray starting at the sensor's current position and continuing in the given angle. The likelihood function which we associate with such observations, shown in Equation \ref{lfunc3}, is a simple Gaussian distribution in azimuth space.\cite{bmtt} Here \(\theta\) is an azimuth observation, \(x \in S\) is an element of the state space, \(\sigma\) is the standard deviation assigned to the observation, and \(b(\alpha,x)\) is a function which gives the azimuth from the sensor position \(\alpha\) to the position \(x\). Applying this function to each particle gives us new weights for the particles with those particles closer to the observed azimuth receving higher weight.

\begin{equation}\label{lfunc3}
L(\theta|x)=(2\pi\sigma^{2})^{\frac{-1}{2}}e^{\frac{-(\theta-b(\alpha,x))^{2}}{2\sigma^{2}}}
\end{equation}

\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{data/particles_azimuth_obs.png}
\caption{Posterior Particle Position Distribution after Azimuth Observation}
\label{posterior1}
\end{figure}

\subsection{Motion Model}
In kinematic problems, observations often occur at different times. In our particular problem, the existance of velocity components in the state space \(S\) indicates that the position components are changing over time. In addition, the possibility that the target might change its velocity must be modeled. Motion updating can be described much more generally and rigorously, but in our case the step is simple. The target is assumed to make instantaneous velocity adjustments with an exponentially distributed mean time between adjustments. During periods where no adjustment is made, the target travels at a constant velocity. If no observations are received for an extended period of time, as has happened in Figure \ref{posterior2}, the particles will drift apart on their constant velocity paths and the posterior distribution will become more difuse to indicate our increasing uncertainty about the current location of the target (since we do not know what it has done since the last observation).

\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{data/particles_motion.png}
\caption{Posterior Particle Position Distribution after Azimuth Observation and Motion Update}
\label{posterior2}
\end{figure}

\section{Design}
Most portions of the Bayesian tracking recursion are embarrisingly parallel operations which map directly to CUDA without much trouble. Initialization of particle positions, time updating, and information updating are all operations on a single particle and are independent of each other particle. However, the CUDA implementation other portions of the algorithm, including particle weight summation (and more generally, particle resampling), and random number generation, provide interesting challenges.

\subsection{Parallel Reduction}
The thrust library provides prewritten parallel algorithms for common CUDA tasks, including reducing an array of values to a single value through repeated application of a binary reduction function.\cite{thrust}

My parallel array summation implementation is based on the NVIDIA white paper on the subject included in the CUDA SDK.\cite{oprc} Understanding the overall summation algorithm requires first understanding the basic shared memory summation algorithm for a single block. A simple fan-in reduction is used. As Figure \ref{sum1} indicates, theads add their partial sums together in a tree pattern until a single partial sum for the entire block remains. Each iteration is synchronized using \verb!__syncthreads()!. Although the values to be summed are passed to the kernel using global memory, each block immediately copies its values to shared memory and performs the rest of the reduction there. Because shared memory is hundreds of times faster than global memory\cite{tutorial1} this strategy is necessary for adequate performance.

\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{data/summation.png}
\caption{Inner-block Reduction In Shared Memory Example}
\label{sum1}
\end{figure}


However, the loop construct that controls this iteration is expensive, as are the \verb!__syncthreads()! calls. Fortunately, because CUDA threads are grouped into \emph{warps} of 32 threads which execute commands simultaniously as a single operation on the GPU, substatial additional savings are possible once the number of partial sums in shared memory drops below 32. By stopping the reduction iteration and unrolling the final iterations, we can avoid synchronization for the iterations where all the summations are occuring simultaniously automatically.

As each block completes its parallel summation, it writes its sum to an array in global memory. Thus, after the first iteration, global memory will contain one partial sum for each block. To sum these values, the kernel is called iteratively until fewer partial sums remain than the number of threads in a single block. At this point, the kernel is called one last time with a single block which writes the final overall sum to global memory.

\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{data/summation_plot.png}
\caption{Array Summation Algorithm Performance}
\label{summation_plot}
\end{figure}

However, as Figure \ref{summation_plot} indicates, writing complex parallel algorithms in cuda efficiently is very difficult. Performance for the serial implementation, custom cuda implementation, and thrust implementation all scale linearly with array size, which makes sense given that the array summation problem has \(O(n)\) time complexity. What is striking is the difference in performance between the custom cuda implementation and the thrust implementation. For the largest arrays tested, the custom cuda implementation was only about twice as fast as the serial implementation whereas the thrust implementation was 12.9 times faster. These results highlight cuda's sensitivity to subtle factors like uncoalesced memory access, shared memory bank conflicts (threads from multiple warps accessing the same sections of shared memory), expensive gpu operations, and other performance gotchas which can have significant impact on performance.\cite{bestprac}

\subsection{Random Number Generation}
Particle filter tracking is a stochastic process: as particles are time updated, they maneuver randomly according to their motion model; as particles are resampled, their replacements are randomly perturbed copies of existing particles. Thus, performing particle filter tracking using CUDA required generating random numbers efficiently on the GPU. The thrust library provides random number generation capabilities, and the CUDA SDK provides a parallel MersenneTwister example. My random number generator implementation is a simpler linear congruential generator which relies on independent seeds stored for each particle.

\begin{equation}\label{lcgeq1}
X_{n+1}=(aX_{n}+c \mod m)
\end{equation}

With properly chosen \(a\), \(c\) and \(m\) values linear congruential generator can provide sufficiently random values for particle filtering\cite{lcg}. The values I used are those used by the java.util.Random class. This approach is attractive because it is theoretically very fast and requires very little state, allowing each particle to generate its own independent stream of random values. Unfortunately, the recurrence relation in equation \ref{lcgeq1} relies on the modulus operator, which is extremely slow on NVIDA hardware.\cite{oprc}

\subsection{Resampling}
As observations arrive and their information about the target's state is incorporated into the prior distribution using appropriate likelihood functions, some particles will match very poorly with the observations, because of their initial state or due to the motion model, and their weight will be very small. Eventually keeping this particle around serves very little purpse, since the Bayesian tracker has already indicated that it matches poorly with the observations and is thus unlikely to be close to the true target state. The effective particle count has been reduced. Resampling is a technique for remedying this situation by periodically repacing particles with very small weight with slightly perturbed copies of particles with very high weight.

\subsubsection{Resampling Implementation 1}

\[ C = \frac{n}{\sum_{i=0}^{n} w_{i}} \]
\begin{equation}\label{resample1}
w_{i}=C w_{j}
\end{equation}

\begin{equation}\label{resample2}
w_{i}=\verb!floor!(C\sum_{j=0}^{i} w_{j})
\end{equation}

Equation \ref{resample1} describes the transformation applied to particle weights \(w_{i}\) to obtain the number of copies to make of each particle. However, to implement the copy operation in parallel in CUDA, each particle must also know where in the global memory particle array that its copies should be placed. Equation \ref{resample2} solves this problem by calculating a cumulative sum of weights. With the cumulative sum in hand, particle \(i\) is copied \(w_{i}-w_{i-1}\) times with the first copy placed at position \(w_{i-1}\) in the global memory array.

\begin{algorithmic}
\STATE $weight_sum\gets Sum Particle Weights$
\end{algorithmic}

\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{data/profile_cuda_version1_pic1.png}
\caption{CUDA Visual Profiler Version 1 Results}
\label{profiler1}
\end{figure}

Unfortunately, as indicated by Figure \ref{profiler1}, this approach is very slow on GPU hardware because of the large amount of uncoalesed memory access which it must perform.

\subsubsection{Resampling Implementation 2}

The second resampling algorithm implementation improves on the first by reducing uncoalesed memory access.

\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{data/profile_cuda_version2_pic1.png}
\caption{CUDA Visual Profiler Version 2 Results}
\label{profiler2}
\end{figure}

\section{Challenges}
Text...

\section{Performance}
Text...

\section{Conclusion}
Text...

\begin{thebibliography}{9}

\bibitem{cpl}
  Brian W. Kernighan and Dennis M. Ritchie,
  \emph{The C Programming Language},
  Prentice Hall PTR, New Jersey,
  2009.

\bibitem{bmtt}
  Stone, Barlow, and Corwin,
  \emph{Bayesian Multiple Target Tracking},
  Artech House, Boston,
  1999.

\bibitem{oprc}
   Harris, Mark,
   \emph{Optimizing Parallel Reduction in CUDA},
   NVIDIA Developer Technology \\
   http://developer.download.nvidia.com/compute/cuda/sdk/website/samples.html

\bibitem{tutorial1}
   Volume I: Introduction to CUDA Programming \\
   http://www.nvidia.com/docs/IO/47904/VolumeI.pdf

\bibitem{bestprac}
   CUDA Best Practices Guide -- CUDA 2.2\\
   http://developer.download.nvidia.com/compute/cuda/2\_3/\\
   toolkit/docs/NVIDIA\_CUDA\_BestPracticesGuide\_2.3.pdf

\bibitem{thrust}
   Thrust C++ Template Library for CUDA \\
   http://code.google.com/p/thrust/

\bibitem{lcg}
   Linear Congruential Generator \\
   http://en.wikipedia.org/wiki/Linear\_congruential\_generator

\end{thebibliography}

\end{document}
