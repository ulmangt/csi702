#import <stdlib.h>
#include <stdio.h>
#include <assert.h>

#include "obs_math.h"
#include "filter_constants.h"

#include <thrust/device_ptr.h>
#include <thrust/reduce.h>

#define MAX_RANGE 20000 // meters
#define MAX_VEL 15 // meters per second

#define DEVICE_PI acosf(-1)

// linear congruential random number generator constants
// these values are used in java.util.Random.next()
#define a 0x5DEECE66DL
#define c 0xB
#define mask_bits 48
// generate only 31 bits so that we always generate positive integers
#define result_bits 31

// bit mask the least significant m bits
const long mask = ( 1L << mask_bits ) - 1;

const long LCG_RAND_MAX = ( 1L << result_bits ) - 1;

// implementation based on examples from:
// http://en.wikipedia.org/wiki/Linear_congruential_generator
// java.util.Random.next()
__device__ int device_lcg_rand( int x )
{
  // bitwise and with mask is equivalent to mod 2^mask_bits
  long xl = (x * a + c) & mask;
  // we have generated mask_bits but only need result_bits
  // use the highest order bits because they have longer periods
  return (int) ( xl >> ( mask_bits - result_bits ) );
}

// Return a random float value evenly distributed between 0 and max
// Because cuda threads will keep track of their own seeds, this function
// and the others like it are not random. They simply take an already
// generated random value (seed) and transform it in some way.
// Seed values should be generated by repeated calls to device_lcg_rand().
__device__ float device_frand0( int seed, float max )
{
  return ( float ) seed / ( float ) LCG_RAND_MAX * max ;
}

// return a random float value evenly distributed between min and max
__device__ float device_frand( int seed, float min, float max )
{
  float diff = max - min;
  return device_frand0( seed, diff ) + min;
}

// return an exponentially distributed random float
__device__ float device_erand( int seed, float inv_lambda )
{
  return -log( device_frand0( seed, 1.0 ) ) * inv_lambda;
}


void checkCUDAError(const char *msg)
{
    cudaError_t err = cudaGetLastError();
    if( cudaSuccess != err) 
    {
        fprintf(stderr, "Cuda error: %s: %s.\n", msg, cudaGetErrorString( err) );
        exit(-1);
    }
}

// CUDA kernel function : time update a particle
__global__ void time_update_kernel( float *d_x_pos, float *d_y_pos, float *d_x_vel, float *d_y_vel, float *d_weight, float *d_seed, float time_sec, float mean_maneuver )
{
  // get the current particle index and retrieve the particle
  int index = blockIdx.x * blockDim.x + threadIdx.x;

  // store the particle's random seed
  float seed = d_seed[index];

  // use the random seed to make a random draw from an exponential distribution with mean time_sec
  // if the draw is larger than mean_maneuver, the particle maneuvers
  if ( device_erand( seed, time_sec ) > mean_maneuver )
  {
    seed = device_lcg_rand( seed );
    d_x_vel[index] = d_x_vel[index] + device_frand( seed, -MAX_VEL_PERTURB, MAX_VEL_PERTURB );
    seed = device_lcg_rand( seed );
    d_y_vel[index] = d_y_vel[index] + device_frand( seed, -MAX_VEL_PERTURB, MAX_VEL_PERTURB );
  }

  // update the random seed and store it back in the particle storage
  d_seed[index] = device_lcg_rand( seed );

  // update the particle's position and velocity
  d_x_pos[index] = d_x_pos[index] + d_x_vel[index] * time_sec;
  d_y_pos[index] = d_y_pos[index] + d_y_vel[index] * time_sec;
}

// updates the position of all particles based on their current velocity
extern "C" void time_update( float *d_x_pos, float *d_y_pos, float *d_x_vel, float *d_y_vel, float *d_weight, float *d_seed, int num, float time_sec, float mean_maneuver )
{
  int numBlocks = num / THREADS_PER_BLOCK;

  // launch kernel
  dim3 dimGrid(numBlocks);
  dim3 dimBlock(THREADS_PER_BLOCK);
  time_update_kernel<<< dimGrid, dimBlock >>>( d_x_pos, d_y_pos, d_x_vel, d_y_vel, d_weight, d_seed, time_sec, mean_maneuver );

  // block until the device has completed kernel execution
  cudaThreadSynchronize();

  // check if the init_particle_val kernel generated errors
  checkCUDAError("time_update");
}

// CUDA kernel function : initialize a particle
__global__ void init_particles_kernel( float *d_x_pos, float *d_y_pos, float *d_x_vel, float *d_y_vel, float *d_weight, float *d_seed )
{
  int index = blockIdx.x * blockDim.x + threadIdx.x;

  int seed = d_seed[index];

  d_x_pos[index]  = device_frand( seed, -MAX_RANGE, MAX_RANGE );
  seed = device_lcg_rand( seed );
  d_y_pos[index]  = device_frand( seed, -MAX_RANGE, MAX_RANGE );
  seed = device_lcg_rand( seed );
  d_x_vel[index]  = device_frand( seed, -MAX_VEL, MAX_VEL );
  seed = device_lcg_rand( seed );
  d_y_vel[index]  = device_frand( seed, -MAX_VEL, MAX_VEL );
  seed = device_lcg_rand( seed );
  d_weight[index] = 1.0;

  d_seed[index] = seed;
}

// modified from reduction code example in CUDA sdk
// num must be a power of 2 for this routine to function
__global__ void sum_weight_array_kernel( float *d_weight_in, float *d_weight_out, int num )
{
  // allocate a shared memory array the size of the current block
  __shared__ float s_shared[THREADS_PER_BLOCK];

  int tid = threadIdx.x; // thread id within block

  // similar to the standard global array index calculation
  // except that blockDimx.x sized spaces are left between
  // the indexes assigned to the threads in each block
  int i = blockIdx.x * ( blockDim.x * 2 ) + threadIdx.x;

  // copy weights from global memory into shared memory for
  // num / 2 threads (only half the threads are from blocks
  // whose i indexes corrispond to actual indexes)
  s_shared[ tid ] = i < num ? d_weight_in[ i ] : 0 ;

  // perform the first stage of the reduction reading from global memory
  // threads in block n add the weight of their corrisponding particle
  // in block n + 1, half the threads sit idle
  if ( i + blockDim.x < num )
  {
    s_shared[ tid ] += d_weight_in[ i + blockDim.x ];
  }

  // wait for s_shared to be fully populated
  __syncthreads();

  // offset tracks the distance between the two entries from
  // s_shared that are added together in the current iteration
  // also, each iteration, offset threads are used
  unsigned int offset = blockDim.x / 2;
  while ( offset > 32 )
  {
    if ( tid < offset )
    {
      s_shared[ tid ] += s_shared[ tid + offset ];
    }

    offset = offset >> 1; // divide offset by 2

    __syncthreads();
  }

  // after offset is 32, all our threads are working within a single warp
  // this means all operations they perform are SIMD (simultanious) and
  // require no __syncthreads()
  // thus, each line corrisponds to an unrolled iteration of the above loop 
  if ( tid < 32 )
  {
    s_shared[ tid ] += s_shared[ tid + 32 ];
    s_shared[ tid ] += s_shared[ tid + 16 ];
    s_shared[ tid ] += s_shared[ tid + 8 ];
    s_shared[ tid ] += s_shared[ tid + 4 ];
    s_shared[ tid ] += s_shared[ tid + 2 ];
    s_shared[ tid ] += s_shared[ tid + 1 ];
  }

  // s_shared[0] now contains the final weight for this block
  // write that result to the global weights array position
  // corrisponding to this block's id
  if ( tid == 0 )
  {
    d_weight_out[ blockIdx.x ] = s_shared[ 0 ];
  }
}

// final reduction routine optimized to work with a single block
__global__ void sum_weight_final_kernel( float *d_weight_in, float *d_weight_out, int num )
{
  // allocate a shared memory array the size of the current block
  __shared__ float s_shared[THREADS_PER_BLOCK];

  int tid = threadIdx.x; // thread id within block

  s_shared[ tid ] = d_weight_in[ tid ] ;

  // offset tracks the distance between the two entries from
  // s_shared that are added together in the current iteration
  // also, each iteration, offset threads are used
  unsigned int offset = blockDim.x / 2;
  while ( offset > 0 )
  {
    if ( tid < offset )
    {
      s_shared[ tid ] += s_shared[ tid + offset ];
    }

    offset = offset >> 1; // divide offset by 2

    __syncthreads();
  }

  // s_shatred[0] now contains the final weight for this block
  // write that result to the global weights array position 0
  if ( tid == 0 )
  {
    d_weight_out[ 0 ] = s_shared[ 0 ];
  }
}

extern "C" float sum_weight( float *d_weights, float *d_temp_weight_in, float *d_temp_weight_out, int num )
{
  int numBlocks = num / THREADS_PER_BLOCK;
  int shared_mem_size = sizeof( float ) * THREADS_PER_BLOCK;

  // copy the particles into a temporary array
  cudaMemcpy( d_temp_weight_in, d_weights, num * sizeof(float), cudaMemcpyDeviceToDevice );

  // each iteration d_temp_weight_out is populated with numBlocks weights
  // for the next iteration, numBlocks is used as numThreads until
  // numBlocks is less than 512, in which case the next iteration
  // can finish the reduction using a single block
  do
  {
    //printf("running kernel blocks: %d num: %d smsize: %d \n", numBlocks, num, shared_mem_size );

    dim3 dimGrid(numBlocks);
    dim3 dimBlock(THREADS_PER_BLOCK);
    sum_weight_array_kernel<<< dimGrid, dimBlock, shared_mem_size >>>( d_temp_weight_in, d_temp_weight_out, num );

    // block until the device has completed kernel execution
    cudaThreadSynchronize();

    // check if the init_particle_val kernel generated errors
    checkCUDAError("sum_weight");

    // for the next iteration, use the out array as the new in array
    float *temp = d_temp_weight_out;
    d_temp_weight_out = d_temp_weight_in;
    d_temp_weight_in = temp;

    // update array size and block count
    num = numBlocks;
    numBlocks = num / THREADS_PER_BLOCK;

    //printf("finished kernel blocks: %d num: %d smsize: %d \n", numBlocks, num, shared_mem_size );
  }
  while( num > THREADS_PER_BLOCK );

  //printf("running final kernel block size: %d \n", numBlocks );

  dim3 dimGridFinal(1);
  dim3 dimBlockFinal(num);
  sum_weight_final_kernel<<< dimGridFinal, dimBlockFinal, shared_mem_size >>>( d_temp_weight_in, d_temp_weight_out, shared_mem_size );

  // block until the device has completed kernel execution
  cudaThreadSynchronize();

  // check if the init_particle_val kernel generated errors
  checkCUDAError("sum_weight");

  float final_sum = 0;

  // each block has reported its sum, now copy those block sums
  // back to the host and sum the blocks weight sums in serial
  // remembering that only half the entries contain partial sums
  cudaMemcpy( &final_sum, d_temp_weight_out, sizeof(float), cudaMemcpyDeviceToHost );

  return final_sum;
}

extern "C" float sum_weight_thrust( float *d_weights, int num )
{
  thrust::device_ptr<float> device_data( d_weights );
  float sum = thrust::reduce(device_data, device_data + num, 0.0f, thrust::plus<float>());
  return sum;
}

// initialize particles, use random seeds to set random positions and velocities
// also set weights of all particles to 1.0
extern "C" void init_particles( float *d_x_pos, float *d_y_pos, float *d_x_vel, float *d_y_vel, float *d_weight, float *d_seed, int num )
{
  int numBlocks = num / THREADS_PER_BLOCK;

  // launch kernel
  dim3 dimGrid(numBlocks);
  dim3 dimBlock(THREADS_PER_BLOCK);
  init_particles_kernel<<< dimGrid, dimBlock >>>( d_x_pos, d_y_pos, d_x_vel, d_y_vel, d_weight, d_seed );

  // block until the device has completed kernel execution
  cudaThreadSynchronize();

  // check if the init_particle_val kernel generated errors
  checkCUDAError("init_particles");
}

// device function to calculate the azimuth between two particles
__device__ float azimuth( float to_x_pos, float to_y_pos, float from_x_pos, float from_y_pos )
{
  float x_diff = from_x_pos - to_x_pos;
  float y_diff = from_y_pos - to_y_pos;

  if ( x_diff == 0 && y_diff > 0 ) return DEVICE_PI / 2.0;
  if ( x_diff == 0 && y_diff < 0 ) return -DEVICE_PI / 2.0;

  return atan2( y_diff, x_diff );
}

// calculates the probability density function for the gaussian
// distribution with given mean and sigma
__device__ float gvalue( float value, float mean, float sigma )
{
  float z = ( value - mean ) / sigma ;
  return exp( -0.5 * z * z ) / ( sqrt( 2.0 * DEVICE_PI ) * sigma );
}

// device function to calculate the distance between two particles
__device__ float range( float to_x_pos, float to_y_pos, float from_x_pos, float from_y_pos )
{
  float x_diff = from_x_pos - to_x_pos;
  float y_diff = from_y_pos - to_y_pos;

  return sqrt( x_diff * x_diff + y_diff * y_diff );
}

// CUDA kernel function : apply an azimuth observation (adjust particle weight)
__global__ void apply_azimuth_observation_kernel( float *d_x_pos, float *d_y_pos, float *d_x_vel, float *d_y_vel, float *d_weight, float *d_seed, float sensor_x_pos, float sensor_y_pos, float obs_value, float obs_error )
{
  int index = blockIdx.x * blockDim.x + threadIdx.x;

  float particle_azimuth = azimuth( sensor_x_pos , sensor_y_pos , d_x_pos[index] , d_y_pos[index] );
  float observed_azimuth = obs_value;
  float likelihood = gvalue( particle_azimuth - observed_azimuth , 0.0 , obs_error );

  d_weight[index] = d_weight[index] * likelihood;
}

// CUDA kernel function : apply a range observation (adjust particle weight)
__global__ void apply_range_observation_kernel( float *d_x_pos, float *d_y_pos, float *d_x_vel, float *d_y_vel, float *d_weight, float *d_seed, float sensor_x_pos, float sensor_y_pos, float obs_value, float obs_error )
{
  int index = blockIdx.x * blockDim.x + threadIdx.x;

  float particle_range = range( sensor_x_pos , sensor_y_pos , d_x_pos[index] , d_y_pos[index] );
  float observed_range = obs_value;
  float likelihood = gvalue( particle_range - observed_range , 0.0 , obs_error );

  d_weight[index] = d_weight[index] * likelihood;
}

// apply observation obs to the particle list, adjusting particle weights
extern "C" void information_update( struct observation *obs, float *d_x_pos, float *d_y_pos, float *d_x_vel, float *d_y_vel, float *d_weight, float *d_seed, int num )
{
  int numBlocks = num / THREADS_PER_BLOCK;

  // launch kernel
  dim3 dimGrid(numBlocks);
  dim3 dimBlock(THREADS_PER_BLOCK);

  switch( obs->type )
  {
    case AZIMUTH:
      apply_azimuth_observation_kernel<<< dimGrid, dimBlock >>>( d_x_pos, d_y_pos, d_x_vel, d_y_vel, d_weight, d_seed, obs->x_pos, obs->y_pos, obs->value, obs->error );
      break;
    case RANGE:
      apply_range_observation_kernel<<< dimGrid, dimBlock >>>( d_x_pos, d_y_pos, d_x_vel, d_y_vel, d_weight, d_seed, obs->x_pos, obs->y_pos, obs->value, obs->error );
      break;
  }

  // block until the device has completed kernel execution
  cudaThreadSynchronize();

  // check if the init_particle_val kernel generated errors
  checkCUDAError("information_update");
}

// copy particles from host (cpu) to device (video card)
extern "C" void copy_array_host_to_device( float *host, float *device, int num )
{
  int size = sizeof( float ) * num;

  cudaMemcpy( device, host, size, cudaMemcpyHostToDevice );
}

// copy particles from device (video card) to host (cpu)
extern "C" void copy_array_device_to_host( float *host, float *device, int num )
{
  int size = sizeof( float ) * num;

  cudaMemcpy( host, device, size, cudaMemcpyDeviceToHost );
}

// allocate memory for num particles on host
extern "C" float* h_init_array_mem( int num )
{
  int size = sizeof( float ) * num ;
  return ( float * ) malloc( size );
}

// allocate memory for num particles on device
extern "C" float* d_init_array_mem( int num )
{
  int size = sizeof( float ) * num ;
  float *array;

  cudaMalloc( (void **) &array, size );

  return array;
}

// free particle memory on host
extern "C" void h_free_particle_mem( float *list )
{
  free( list );
}

// free particle memory on device
extern "C" void d_free_particle_mem( float *list )
{
  cudaFree( list );
}






